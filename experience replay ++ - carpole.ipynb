{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized Replay Buffer ++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrioritizedBuffer(object):\n",
    "    def __init__(self, capacity, prob_alpha=0.6):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity   = capacity\n",
    "        self.buffer     = []\n",
    "        self.pos        = 0\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.states_count = {0:0, 1:0, 'frac': 0}\n",
    "        self.states_loss = {0:0, 1:0, 'frac': 0}\n",
    "    \n",
    "    def push(self, meta_state, action, reward, meta_next_state, done):\n",
    "        state = meta_state[0] \n",
    "        state_env = meta_state[1] \n",
    "        next_state = meta_next_state[0]\n",
    "        \n",
    "        assert state.ndim == next_state.ndim\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        \n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done, int(state_env)))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done, int(state_env))\n",
    "        \n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        \n",
    "        probs  = prios ** self.prob_alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        total    = len(self.buffer)\n",
    "        weights  = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights  = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        batch       = list(zip(*samples))\n",
    "        states      = np.concatenate(batch[0])\n",
    "        actions     = batch[1]\n",
    "        rewards     = batch[2]\n",
    "        next_states = np.concatenate(batch[3])\n",
    "        dones       = batch[4]\n",
    "        state_envs = list(batch[5])\n",
    "        \n",
    "        # increment states count\n",
    "        self.states_count[0] += len(state_envs) - sum(state_envs) # states.shape[0] - np.sum(states[:,-1])\n",
    "        self.states_count[1] += sum(state_envs)  # np.sum(states[:,-1])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta_start = 0.4\n",
    "beta_frames = 1000 \n",
    "beta_by_frame = lambda frame_idx: min(1.0, beta_start + frame_idx * (1.0 - beta_start) / beta_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v0\"\n",
    "env = gym.make(env_id)\n",
    "val_env = gym.make(env_id)\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "        )\n",
    "        self.fc = nn.Linear(128, env.action_space.n)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        out = self.fc(F.relu(out))\n",
    "        return out\n",
    "    \n",
    "    def get_hidden(self, x):\n",
    "        hidden = self.layers(x)\n",
    "        return hidden\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "            action = int(action)\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_model = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "target_model  = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    current_model = current_model.cuda()\n",
    "    target_model  = target_model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(current_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_target(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update_target(current_model, target_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_td_loss(cur_model, tar_model, batch_size, beta):\n",
    "    state, action, reward, next_state, done, indices, weights = replay_buffer.sample(batch_size, beta)\n",
    "    \n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "    weights    = Variable(torch.FloatTensor(weights))\n",
    "\n",
    "    q_values      = cur_model(state)\n",
    "    next_q_values = tar_model(next_state)\n",
    "\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss  = (q_value - expected_q_value.detach()).pow(2) * weights\n",
    "    prios = loss + 1e-5\n",
    "    loss  = loss.mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    replay_buffer.update_priorities(indices, prios.data.cpu().numpy())\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses, standard_val_rewards, noisy_val_rewards, states_count_ratios):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,16))\n",
    "    plt.subplot(321)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(322)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.subplot(323)\n",
    "    plt.title('frame %s. val reward on standard env: %s' % (frame_idx, np.mean(standard_val_rewards[-10:])))\n",
    "    plt.plot(standard_val_rewards)\n",
    "    plt.subplot(324)\n",
    "    plt.title('frame %s.val reward on noisy env: %s' % (frame_idx, np.mean(noisy_val_rewards[-10:])))\n",
    "    plt.plot(noisy_val_rewards)\n",
    "    plt.subplot(325)\n",
    "    plt.title('frame %s. proportion of selecting noisy env: %s' % (frame_idx, np.mean(states_count_ratios[-10:])))\n",
    "    plt.plot(states_count_ratios)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test agent on standard or noisy env\n",
    "# Resulting episode reward stored in XXX_val_rewards where XXX is standard or noisy \n",
    "num_val_trials = 10\n",
    "def test(noisyGame, eps):\n",
    "    rewards = []\n",
    "    for i in range(num_val_trials):\n",
    "        epsilon = 0 \n",
    "        episode_reward = 0\n",
    "        state = val_env.reset()\n",
    "        state = np.append(state, float(noisyGame)) # BACK IN \n",
    "        with torch.no_grad():\n",
    "            while True:\n",
    "                original_action = current_model.act(state, epsilon)\n",
    "                \n",
    "                if original_action != int(original_action):\n",
    "                    original_action = original_action.numpy()[0]\n",
    "\n",
    "                if noisyGame and random.uniform(0,1) < eps:\n",
    "                    actual_action = 1 - original_action\n",
    "                else:\n",
    "                    actual_action = original_action \n",
    "\n",
    "                next_state, reward, done, _ = val_env.step(actual_action)\n",
    "                next_state = np.append(next_state, float(noisyGame))\n",
    "                \n",
    "                if noisyGame:\n",
    "                    reward += random.uniform(-1., 1.)\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(episode_reward)\n",
    "                    break \n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e64e20351240818b1b01d45eadee31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/megumisano/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prop of selecting noisy env 0.7979565860754414\n"
     ]
    }
   ],
   "source": [
    "num_frames = 10000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "standard_val_rewards = [] \n",
    "noisy_val_rewards = [] \n",
    "states_count_ratios = []\n",
    "episode_reward = 0\n",
    "\n",
    "noisyGame = False\n",
    "state = env.reset()\n",
    "state = np.append(state, float(noisyGame)) # BACK IN \n",
    "meta_state = (state, float(noisyGame))\n",
    "# zerod_state = np.append(state[:-1] ,0) #np.append(state, 0)\n",
    "replay_buffer = PrioritizedBuffer(100000)\n",
    "current_model = DQN(env.observation_space.shape[0] + 1, env.action_space.n) # BACK IN \n",
    "target_model  = DQN(env.observation_space.shape[0] + 1, env.action_space.n) # BACK IN\n",
    "optimizer = optim.Adam(current_model.parameters())\n",
    "\n",
    "# Probability of action being random in noisy state\n",
    "eps = 1.\n",
    "f = FloatProgress(min=0, max=num_frames)\n",
    "display(f)\n",
    "\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    # env.env.viewer.window.dispatch_events()\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    original_action = current_model.act(state, epsilon)\n",
    "    \n",
    "    # If in noisy environment, make action random with probability eps \n",
    "    if noisyGame and random.uniform(0,1) < eps:\n",
    "        actual_action = original_action # invert \n",
    "    else:\n",
    "        actual_action = original_action\n",
    "    next_state, reward, done, _ = env.step(actual_action)\n",
    "    \n",
    "    # If in noisy environment, make reward completely random \n",
    "    if noisyGame:\n",
    "        reward *= random.uniform(-1., 1.)\n",
    "    next_state = np.append(next_state, float(noisyGame)) # BACK IN \n",
    "    meta_next_state = (next_state, float(noisyGame))\n",
    "    #zerod_state = np.append(next_state[:-1], 0)\n",
    "    #replay_buffer.push(state, original_action, reward, next_state, done)\n",
    "    replay_buffer.push(meta_state, original_action, reward, meta_next_state, done)\n",
    "\n",
    "    meta_state = meta_next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        noisyGame = 1-noisyGame\n",
    "        state = env.reset()\n",
    "        state = np.append(state, float(noisyGame)) # BACK IN \n",
    "        meta_state = (state, float(noisyGame))\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > batch_size:\n",
    "        beta = beta_by_frame(frame_idx)\n",
    "        loss = compute_td_loss(current_model, target_model, batch_size, beta)\n",
    "        losses.append(loss.data.tolist())\n",
    "        \n",
    "    if frame_idx % 200 == 0:\n",
    "        standard_val_rewards.append(test(False, eps))\n",
    "        noisy_val_rewards.append(test(True, eps))\n",
    "        states_count_ratios.append(float(replay_buffer.states_count[1]) / (float(replay_buffer.states_count[1])  + float(replay_buffer.states_count[0])))\n",
    "#         plot(frame_idx, all_rewards, losses, standard_val_rewards, noisy_val_rewards, states_count_ratios)\n",
    "        \n",
    "    if frame_idx % 1000 == 0:\n",
    "        update_target(current_model, target_model)\n",
    "        \n",
    "    f.value += 1\n",
    "        \n",
    "print(\"prop of selecting noisy env\", str(float(replay_buffer.states_count[1]) / (float(replay_buffer.states_count[1])  + float(replay_buffer.states_count[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flipping control isn't necessary \n",
    "# including noisyGame in state is necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wrappers import make_atari, wrap_deepmind, wrap_pytorch # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)\n",
    "val_env    = make_atari(env_id)\n",
    "val_env    = wrap_deepmind(val_env)\n",
    "val_env    = wrap_pytorch(val_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "#     def get_hidden(self, x):\n",
    "#         hidden = self.layers(x)\n",
    "#         return hidden\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_model = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "target_model  = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    current_model = current_model.cuda()\n",
    "    target_model  = target_model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(current_model.parameters(), lr=0.0001)\n",
    "\n",
    "replay_initial = 10000\n",
    "replay_buffer  = PrioritizedBuffer(100000)\n",
    "\n",
    "update_target(current_model, target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1398a3780>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD/CAYAAAD8MdEiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH8zz0i9/Z0g4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([epsilon_by_frame(i) for i in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test agent on standard or noisy env\n",
    "# Resulting episode reward stored in XXX_val_rewards where XXX is standard or noisy \n",
    "num_val_trials = 10\n",
    "def test(noisyGame, eps):\n",
    "    rewards = []\n",
    "    for i in range(num_val_trials):\n",
    "        epsilon = 0 \n",
    "        episode_reward = 0\n",
    "        state = val_env.reset()\n",
    "        state[:,:,-1] = float(noisyGame)\n",
    "        with torch.no_grad():\n",
    "            while True:\n",
    "                original_action = current_model.act(state, epsilon)\n",
    "                \n",
    "                if original_action != int(original_action):\n",
    "                    original_action = original_action.numpy()[0]\n",
    "\n",
    "                if noisyGame and random.uniform(0,1) < eps:\n",
    "                    actual_action = 1 - original_action\n",
    "                else:\n",
    "                    actual_action = original_action \n",
    "\n",
    "                next_state, reward, done, _ = val_env.step(actual_action)\n",
    "                next_state[:,:,-1] = float(noisyGame)\n",
    "                \n",
    "                if noisyGame:\n",
    "                    reward += random.uniform(-1., 1.)\n",
    "                #next_state = np.append(next_state, float(noisyGame))\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(episode_reward)\n",
    "                    break \n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state (1, 84, 84)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a1382a996b41bfadeeef50ea1e5e32"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/megumisano/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prop of selecting noisy env 0.3614942817014446\n"
     ]
    }
   ],
   "source": [
    "num_frames = 10000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "standard_val_rewards = [] \n",
    "noisy_val_rewards = [] \n",
    "states_count_ratios = []\n",
    "episode_reward = 0\n",
    "\n",
    "noisyGame = False\n",
    "state = env.reset()\n",
    "print(\"state\", state.shape)\n",
    "state[:,:,-1] = float(noisyGame)\n",
    "meta_state = (state, float(noisyGame))\n",
    "replay_buffer = PrioritizedBuffer(100000)\n",
    "current_model = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "target_model  = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "optimizer = optim.Adam(current_model.parameters())\n",
    "\n",
    "# Probability of action being random in noisy state\n",
    "eps = 1.\n",
    "f = FloatProgress(min=0, max=num_frames)\n",
    "display(f)\n",
    "\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    # env.env.viewer.window.dispatch_events()\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "#     original_action = current_model.act(state, epsilon)\n",
    "    original_action = current_model.act(state, epsilon)\n",
    "    \n",
    "    # If in noisy environment, make action random with probability eps \n",
    "    if noisyGame and random.uniform(0,1) < eps:\n",
    "        actual_action = original_action # invert \n",
    "    else:\n",
    "        actual_action = original_action\n",
    "    next_state, reward, done, _ = env.step(actual_action)\n",
    "    next_state[:,:,-1] = float(noisyGame)\n",
    "    next_meta_state = (next_state, noisyGame)\n",
    "    # If in noisy environment, make reward completely random \n",
    "    if noisyGame:\n",
    "        reward *= random.uniform(-1., 1.)\n",
    "    #next_state = np.append(next_state, float(noisyGame))\n",
    "    #zerod_state = np.append(next_state[:-1], 0)\n",
    "    #replay_buffer.push(state, original_action, reward, next_state, done)\n",
    "    replay_buffer.push(meta_state, original_action, reward, next_meta_state, done)\n",
    "\n",
    "    meta_state = next_meta_state \n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        noisyGame = 1-noisyGame\n",
    "        state = env.reset()\n",
    "        state[:,:,-1] = float(noisyGame)\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > batch_size:\n",
    "        beta = beta_by_frame(frame_idx)\n",
    "        loss = compute_td_loss(current_model, target_model, batch_size, beta)\n",
    "        losses.append(loss.data.tolist())\n",
    "        \n",
    "    if frame_idx % 200 == 0:\n",
    "        standard_val_rewards.append(test(False, eps))\n",
    "        noisy_val_rewards.append(test(True, eps))\n",
    "        states_count_ratios.append(float(replay_buffer.states_count[1]) / (float(replay_buffer.states_count[1])  + float(replay_buffer.states_count[0])))\n",
    "#         plot(frame_idx, all_rewards, losses, standard_val_rewards, noisy_val_rewards, states_count_ratios)\n",
    "        \n",
    "    if frame_idx % 1000 == 0:\n",
    "        update_target(current_model, target_model)\n",
    "        \n",
    "    f.value += 1\n",
    "        \n",
    "print(\"prop of selecting noisy env\", str(float(replay_buffer.states_count[1]) / (float(replay_buffer.states_count[1])  + float(replay_buffer.states_count[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 84, 84)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state[-1,-1,-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
