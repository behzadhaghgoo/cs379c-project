{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Set up \n",
    "!pip install torch torchvision gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    print(\"Using\", torch.cuda.device_count() ,\"GPUs.\")\n",
    "    gpu_ids = list(range(torch.cuda.device_count()))\n",
    "else:\n",
    "    print(\"Using CPU.\")\n",
    "    gpu_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prioritized Buffer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedBuffer(object):\n",
    "    def __init__(self, capacity, prob_alpha= 0.6):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity   = capacity\n",
    "        self.buffer     = []\n",
    "        self.pos        = 0\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.states_count = {0:0, 1:0, 'frac': 0}\n",
    "        self.states_loss = {0:0, 1:0, 'frac': 0}\n",
    "        self.sample_counts = np.zeros(capacity)\n",
    "\n",
    "    def push(self, meta_state, action, reward, meta_next_state, done):\n",
    "        state = meta_state[0]\n",
    "        state_env = meta_state[1]\n",
    "        next_state = meta_next_state[0]\n",
    "\n",
    "        assert state.ndim == next_state.ndim\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done, int(state_env)))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done, int(state_env))\n",
    "\n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        probs  = prios ** self.prob_alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        self.sample_counts[indices] += 1\n",
    "\n",
    "        total    = len(self.buffer)\n",
    "        weights  = (total * probs[indices]) ** (-beta)\n",
    "        weights /= (weights.max()) # FIXIT: What if the max is zero?\n",
    "        weights  = np.array(weights, dtype=np.float32)\n",
    "\n",
    "        batch       = list(zip(*samples))\n",
    "        states      = np.concatenate(batch[0])\n",
    "        actions     = batch[1]\n",
    "        rewards     = batch[2]\n",
    "        next_states = np.concatenate(batch[3])\n",
    "        dones       = batch[4]\n",
    "        state_envs = list(batch[5])\n",
    "\n",
    "        # increment states count\n",
    "        self.states_count[0] += len(state_envs) - sum(state_envs) # states.shape[0] - np.sum(states[:,-1])\n",
    "        self.states_count[1] += sum(state_envs)  # np.sum(states[:,-1])\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, indices, weights, state_envs\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n",
    "            \n",
    "    def get_average_weight_by_env(self):\n",
    "        # may be worth switching to np array buffer for efficiency\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        probs  = prios ** self.prob_alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        noisy_weight_sum = 0\n",
    "        std_weight_sum = 0\n",
    "        noisy_count = 0\n",
    "        for i in range(len(self.buffer)):\n",
    "            state = self.buffer[i][0]\n",
    "            is_noisy_state = (state.squeeze()[-1] == 1)\n",
    "            if is_noisy_state:\n",
    "                noisy_weight_sum += probs[i]\n",
    "                noisy_count += 1\n",
    "            else:\n",
    "                std_weight_sum += probs[i]\n",
    "                \n",
    "        return {'std_avg': std_weight_sum / (len(self.buffer) - noisy_count),\n",
    "                'noisy_avg': noisy_weight_sum / noisy_count,\n",
    "                'noisy_count': noisy_count,\n",
    "                'std_count': len(self.buffer) - noisy_count}\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_start = 0.4\n",
    "beta_frames = 1000\n",
    "beta_by_frame = lambda frame_idx: min(1.0, beta_start + frame_idx * (1.0 - beta_start) / beta_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v0\"\n",
    "env = gym.make(env_id)\n",
    "val_env = gym.make(env_id)\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "        )\n",
    "        self.fc = nn.Linear(128, env.action_space.n)\n",
    "\n",
    "    def forward(self, x, return_latent = 'last'):\n",
    "        \"\"\"Args:\n",
    "        \t return_latent: 'last': return last hidden vector\n",
    "           \t\t\t\t\t\t\t\t'state': return the state\n",
    "        \"\"\"\n",
    "        hidden = self.layers(x)\n",
    "        out = self.fc(F.relu(hidden))\n",
    "        if return_latent == \"state\":\n",
    "          \treturn out, state\n",
    "        return out, hidden \n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)\n",
    "            q_value,_  = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "            action = int(action)\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TD Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDLoss():\n",
    "\n",
    "    def __init__(self,batch_size = 32, theta = 1, mode = \"dot\", exp = False, meg_norm = False, average_q_values = False):\n",
    "        \"\"\"Args:\n",
    "         mode: \"dot\" or \"euc\", the distance function for averaging\n",
    "         theta: power of weights (see paper) \"\"\"\n",
    "        super(TDLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.theta = theta\n",
    "        self.mode = mode\n",
    "        self.exp = exp\n",
    "        self.meg_norm = meg_norm\n",
    "        self.hidden = \"hidden\"\n",
    "        self.average_q_values = average_q_values\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def hidden_weights(self, h):\n",
    "        # TODO: rename normalized norm for clarity, implement normalized and unnormalized version (unnormalized\n",
    "        # seems to be standard in CS224n coverage of attention)\n",
    "        if self.meg_norm:\n",
    "          # Meg Norm: w[i,j] = h[i].h[j]/|h[i]||h[j]|\n",
    "          h = h / torch.reshape(torch.norm(torch.mul(h, h), dim = 1, p = 2), (-1,1))\n",
    "        \n",
    "        weights = torch.mm(h,torch.transpose(h, 0, 1))\n",
    "        # Make weights positive and normalize with softmax (maps (-inf, inf) to [0,1] while maintaining ordering)\n",
    "        weights = torch.nn.functional.softmax(weights, dim=1)**self.theta\n",
    "        return weights\n",
    "\n",
    "    def hidden_mean(self, h, tensor):\n",
    "        if self.exp:\n",
    "            tensor = torch.exp(tensor)\n",
    "        tensor = torch.reshape(tensor,(-1,1))\n",
    "\n",
    "        if self.mode == \"dot\":\n",
    "            weights = self.hidden_weights(h)\n",
    "        elif self.mode == \"euc\":\n",
    "            # TODO: Implement euclidean_weights\n",
    "            raise Exception(\"euclidean_weights not implemented\")\n",
    "\n",
    "        output = torch.mm(weights, tensor)\n",
    "        output = output.squeeze(1)\n",
    "        if self.exp:\n",
    "            return torch.log(output * self.batch_size)\n",
    "        return output * self.batch_size\n",
    "\n",
    "    def compute_td_loss(self, cur_model, tar_model, beta, replay_buffer, optimizer):\n",
    "        state, action, reward, next_state, done, indices, weights, state_envs = replay_buffer.sample(self.batch_size, beta)\n",
    "\n",
    "        state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "        next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "        action     = Variable(torch.LongTensor(action))\n",
    "        reward     = Variable(torch.FloatTensor(reward))\n",
    "        done       = Variable(torch.FloatTensor(done))\n",
    "        weights    = Variable(torch.FloatTensor(weights))\n",
    "        \n",
    "        # predict q value and store hidden state if averaging q values\n",
    "        if self.average_q_values:\n",
    "            q_values, hiddens = cur_model.forward(state, return_latent = \"last\")\n",
    "        else:\n",
    "            q_values, hiddens = cur_model.forward(state, return_latent = None)\n",
    "        next_q_values, _ = tar_model(next_state)\n",
    "\n",
    "        q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        next_q_value     = next_q_values.max(1)[0]\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "\n",
    "        loss  = (q_value - expected_q_value.detach()).pow(2) * weights\n",
    "        loss  = loss.mean()\n",
    "\n",
    "        if self.average_q_values:\n",
    "            # TODO: computing average over only sampled hidden states is limiting. Ideal case: compute over\n",
    "            # entire buffer each time. More realistic, have buffer that stores hidden state (limits size of buffer,\n",
    "            # risk of stale entries if large). Interesting spot for experimentation\n",
    "            q_value = self.hidden_mean(hiddens, q_value)\n",
    "            # should not be averaging expected q value--the goal of the algorithm is to have each individual\n",
    "            # prediction be close to the averaged prediction\n",
    "            # possible issue with task as it is defined now: it is beneficial to average noisy tasks with non-noisy\n",
    "            # (though not the reverse)\n",
    "            # TODO: investigate hidden state similarity\n",
    "#              expected_q_value = self.hidden_mean(hiddens, expected_q_value)\n",
    "\n",
    "        prios = (q_value - expected_q_value.detach()).pow(2) * weights + 1e-5\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        replay_buffer.update_priorities(indices, prios.data.cpu().numpy())\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test agent on standard or noisy env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test agent on standard or noisy env\n",
    "# Resulting episode reward stored in XXX_val_rewards where XXX is standard or noisy\n",
    "def test(noisyGame, eps, num_val_trials, current_model):\n",
    "    rewards = []\n",
    "    for i in range(num_val_trials):\n",
    "        epsilon = 0\n",
    "        episode_reward = 0\n",
    "        state = val_env.reset()\n",
    "        state = np.append(state, float(noisyGame))\n",
    "        with torch.no_grad():\n",
    "            while True:\n",
    "                original_action = current_model.act(state, epsilon)\n",
    "\n",
    "                if original_action != int(original_action):\n",
    "                    original_action = original_action.numpy()[0]\n",
    "\n",
    "                actual_action = original_action\n",
    "                next_state, reward, done, _ = val_env.step(actual_action)\n",
    "                next_state = np.append(next_state, float(noisyGame))\n",
    "\n",
    "                if noisyGame:\n",
    "                    reward += random.uniform(-1., 1.)\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(episode_reward)\n",
    "                    break\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(variance, theta, exp, average_q_values, meg_norm, hardcoded, invert_actions = False, num_frames = 10000, num_val_trials = 10, batch_size = 32, gamma = 0.99, num_trials = 5, USE_CUDA = False, device = \"\", eps = 1.):\n",
    "\n",
    "    # Progress bar\n",
    "    f = FloatProgress(min=0, max=num_frames)\n",
    "    display(f)\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    \"\"\"Args:\"\"\"\n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    standard_val_rewards = []\n",
    "    noisy_val_rewards = []\n",
    "    states_count_ratios = []\n",
    "    episode_reward = 0\n",
    "\n",
    "    # Initialize state\n",
    "    noisyGame = False\n",
    "    state = env.reset()\n",
    "    state = np.append(state, float(noisyGame)) # BACK IN\n",
    "    meta_state = (state, float(noisyGame))\n",
    "\n",
    "    # Initialize replay buffer, model, TD loss, and optimizers\n",
    "    replay_buffer = PrioritizedBuffer(100000)\n",
    "    # replay_buffer = PrioritizedBuffer(1000)\n",
    "    current_model = DQN(env.observation_space.shape[0] + 1, env.action_space.n) # BACK IN\n",
    "    target_model  = DQN(env.observation_space.shape[0] + 1, env.action_space.n) # BACK IN\n",
    "    td_loss = TDLoss(average_q_values=average_q_values)\n",
    "    optimizer = optim.Adam(current_model.parameters())\n",
    "    \n",
    "    # Multi GPU - Under Construction.\n",
    "#     current_model = current_model.to(device)\n",
    "#     target_model = target_model.to(device)\n",
    "\n",
    "#     # Single GPU Code\n",
    "    if USE_CUDA:\n",
    "        current_model = nn.DataParallel(current_model, gpu_ids)\n",
    "        target_model = nn.DataParallel(target_model, gpu_ids)\n",
    "        current_model = current_model.cuda()\n",
    "        target_model  = target_model.cuda()\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "    power = theta\n",
    "    var = variance\n",
    "    all_standard_val_rewards = []\n",
    "    all_proportions = []\n",
    "    std_weights = []\n",
    "    noisy_weights = []\n",
    "    std_buffer_example_count = []\n",
    "    noisy_buffer_example_count = [] \n",
    "    \n",
    "    eps = .5\n",
    "\n",
    "    for t in range(num_trials):\n",
    "        print(\"trial number: {}\".format(t))\n",
    "        #print(\"is noisy game?\", noisyGame)\n",
    "        for frame_idx in range(1, num_frames + 1):\n",
    "            epsilon = epsilon_by_frame(frame_idx)\n",
    "            original_action = current_model.act(state, epsilon)\n",
    "\n",
    "            # If in noisy environment, make action random with probability eps\n",
    "            if noisyGame and random.uniform(0,1) < eps:\n",
    "                if invert_actions:\n",
    "                    actual_action = 1 - original_action # invert\n",
    "                else:\n",
    "                    actual_action = original_action\n",
    "            else:\n",
    "                actual_action = original_action\n",
    "\n",
    "            next_state, reward, done, _ = env.step(actual_action)\n",
    "\n",
    "            # If in noisy environment, make reward completely random\n",
    "            if noisyGame:\n",
    "                reward *= np.random.normal(0, var)\n",
    "\n",
    "            next_state = np.append(next_state, float(noisyGame))\n",
    "            meta_next_state = (next_state, float(noisyGame))\n",
    "            replay_buffer.push(meta_state, original_action, reward, meta_next_state, done)\n",
    "\n",
    "            meta_state = meta_next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                noisyGame = 1-noisyGame\n",
    "                state = env.reset()\n",
    "                state = np.append(state, float(noisyGame))\n",
    "                meta_state = (state, float(noisyGame))\n",
    "                all_rewards.append(episode_reward)\n",
    "                episode_reward = 0\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                beta = beta_by_frame(frame_idx)\n",
    "                loss = td_loss.compute_td_loss(current_model, target_model, beta, replay_buffer, optimizer)\n",
    "                losses.append(loss.data.tolist())\n",
    "\n",
    "            if frame_idx % 200 == 0:\n",
    "                all_standard_val_rewards.append(test(False, eps, num_val_trials, current_model))\n",
    "                all_proportions.append(float(replay_buffer.states_count[1]) / (float(replay_buffer.states_count[1])  + float(replay_buffer.states_count[0])))\n",
    "                weight_dict = replay_buffer.get_average_weight_by_env()\n",
    "                std_weights.append(weight_dict['std_avg'])\n",
    "                noisy_weights.append(weight_dict['noisy_avg'])\n",
    "                std_buffer_example_count.append(weight_dict['std_count'])\n",
    "                noisy_buffer_example_count.append(weight_dict['noisy_count'])\n",
    "                \n",
    "                # Verified that standard experience replay weights noisy examples more heavily and oversamples\n",
    "                # TODO: run complete experiments and check improvement with proposed replay technique\n",
    "#                 print(weight_dict)\n",
    "#                 print('Noisy to std weight', noisy_weights[-1]/std_weights[-1])\n",
    "#                 print('Proportion of noisy selected (long run): ', all_proportions[-1])\n",
    "                \n",
    "            #         plot(frame_idx, all_rewards, losses, standard_val_rewards, noisy_val_rewards, states_count_ratios)\n",
    "\n",
    "            if frame_idx % 1000 == 0:\n",
    "                update_target(current_model, target_model)\n",
    "                print(\"frame_idx\", frame_idx)\n",
    "\n",
    "            f.value += 1\n",
    "\n",
    "    result_df['trial_num'] = list(range(50)) * num_trials # 50 = 10000 frames / 200 frams\n",
    "    result_df['val_reward'] = all_standard_val_rewards\n",
    "    result_df['proportion'] = all_proportions\n",
    "    result_df['std_weights'] = std_weights\n",
    "    result_df['noisy_weights'] = noisy_weights\n",
    "    result_df['std_buffer_example_count'] = std_buffer_example_count\n",
    "    result_df['noisy_buffer_example_count'] = noisy_buffer_example_count\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(variances = np.logspace(0.1, 10, 4), thetas = np.logspace(0.1, 5, 4), exps = [False], meg_norms = [True, False], hardcodeds=[True, False], average_q_values=[False]):\n",
    "    meta_dict = {}\n",
    "    #pd.DataFrame(columns=['var', 'theta', 'exp', 'meg_norm', 'result_df'])\n",
    "    for var in variances:\n",
    "        for theta in thetas:\n",
    "            for exp in exps:\n",
    "                for avg_q in average_q_values:\n",
    "                    for meg_norm in meg_norms:\n",
    "                        for hardcoded in hardcodeds:\n",
    "                            result_df = train(var, theta, exp, avg_q, meg_norm, hardcoded, invert_actions = False, num_frames = 10000, num_val_trials = 10, batch_size = 32, num_trials = 5, USE_CUDA = USE_CUDA)\n",
    "                            key = [var, theta, exp, meg_norm, hardcoded]\n",
    "                            meta_dict[key] = result_df\n",
    "    return meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_dict = run_experiments(variances = np.logspace(0.1, 10, 4), \n",
    "#                             thetas = np.logspace(0.1, 5, 4), \n",
    "#                             exps = [False], \n",
    "#                             meg_norms = [True, False], \n",
    "#                             hardcodeds=[True, False])\n",
    "\n",
    "meta_dict = run_experiments(variances = [.5], \n",
    "                            thetas = [1], \n",
    "                            exps = [False], \n",
    "                            meg_norms = [False], \n",
    "                            hardcodeds=[False],\n",
    "                            average_q_values=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
