{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Set up \n",
    "!pip install torch torchvision gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import metadict_save as ms\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    print(\"Using\", torch.cuda.device_count() ,\"GPUs.\")\n",
    "    gpu_ids = list(range(torch.cuda.device_count()))\n",
    "else:\n",
    "    print(\"Using CPU.\")\n",
    "    gpu_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_start = 0.4\n",
    "beta_frames = 1000\n",
    "beta_by_frame = lambda frame_idx: min(1.0, beta_start + frame_idx * (1.0 - beta_start) / beta_frames)\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test agent on standard or noisy env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test agent on standard or noisy env\n",
    "# Resulting episode reward stored in XXX_val_rewards where XXX is standard or noisy\n",
    "def test(val_env, noisyGame, eps, num_val_trials, current_model):\n",
    "    rewards = []\n",
    "    for i in range(num_val_trials):\n",
    "        epsilon = 0\n",
    "        episode_reward = 0\n",
    "        state = val_env.reset()\n",
    "        state = np.append(state, float(noisyGame))\n",
    "        with torch.no_grad():\n",
    "            while True:\n",
    "                original_action = current_model.act(state, epsilon)\n",
    "\n",
    "                if original_action != int(original_action):\n",
    "                    original_action = original_action.numpy()[0]\n",
    "\n",
    "                actual_action = original_action\n",
    "                next_state, reward, done, _ = val_env.step(actual_action)\n",
    "                next_state = np.append(next_state, float(noisyGame))\n",
    "\n",
    "                if noisyGame:\n",
    "                    reward += random.uniform(-1., 1.)\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(episode_reward)\n",
    "                    break\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, val_env, variance, theta, exp, average_q_values, meg_norm, hardcoded, invert_actions = False, num_frames = 10000, num_val_trials = 10, batch_size = 32, gamma = 0.99, num_trials = 5, USE_CUDA = False, device = \"\", eps = 1.):\n",
    "\n",
    "    num_trials = 1\n",
    "    \n",
    "    # Progress bar\n",
    "    f = FloatProgress(min=0, max=num_frames)\n",
    "    display(f)\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    \"\"\"Args:\"\"\"\n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    standard_val_rewards = []\n",
    "    noisy_val_rewards = []\n",
    "    states_count_ratios = []\n",
    "    episode_reward = 0\n",
    "\n",
    "    # Initialize state\n",
    "    noisyGame = False\n",
    "    state = env.reset()\n",
    "    state = np.append(state, float(noisyGame)) # BACK IN\n",
    "    meta_state = (state, float(noisyGame))\n",
    "\n",
    "    # Initialize replay buffer, model, TD loss, and optimizers\n",
    "    replay_buffer = PrioritizedBuffer(100000)\n",
    "    # replay_buffer = PrioritizedBuffer(1000)\n",
    "    current_model = DQN(env.observation_space.shape[0] + 1, env.action_space.n) # BACK IN\n",
    "    target_model  = DQN(env.observation_space.shape[0] + 1, env.action_space.n) # BACK IN\n",
    "    td_loss = TDLoss(average_q_values=average_q_values)\n",
    "    optimizer = optim.Adam(current_model.parameters())\n",
    "    \n",
    "    # Multi GPU - Under Construction.\n",
    "#     current_model = current_model.to(device)\n",
    "#     target_model = target_model.to(device)\n",
    "\n",
    "#     # Single GPU Code\n",
    "    if USE_CUDA:\n",
    "        current_model = nn.DataParallel(current_model, gpu_ids)\n",
    "        target_model = nn.DataParallel(target_model, gpu_ids)\n",
    "        current_model = current_model.cuda()\n",
    "        target_model  = target_model.cuda()\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "    power = theta\n",
    "    var = variance\n",
    "    all_standard_val_rewards = []\n",
    "    all_proportions = []\n",
    "    std_weights = []\n",
    "    noisy_weights = []\n",
    "    std_buffer_example_count = []\n",
    "    noisy_buffer_example_count = []\n",
    "    \n",
    "    # TODO: restructure code to incorporate epsilon as experiment hyperparam\n",
    "    # eps=.5\n",
    "\n",
    "    for t in range(num_trials):\n",
    "        print(\"trial number: {}\".format(t))\n",
    "        #print(\"is noisy game?\", noisyGame)\n",
    "        for frame_idx in range(1, num_frames + 1):\n",
    "            epsilon = epsilon_by_frame(frame_idx)\n",
    "            original_action = current_model.act(state, epsilon)\n",
    "\n",
    "            # If in noisy environment, make action random with probability eps\n",
    "            if noisyGame and random.uniform(0,1) < eps:\n",
    "                if invert_actions:\n",
    "                    actual_action = 1 - original_action # invert\n",
    "                else:\n",
    "                    actual_action = original_action\n",
    "            else:\n",
    "                actual_action = original_action\n",
    "\n",
    "            next_state, reward, done, _ = env.step(actual_action)\n",
    "\n",
    "            # If in noisy environment, make reward completely random\n",
    "            if noisyGame:\n",
    "                reward *= np.random.normal(0, var)\n",
    "\n",
    "            next_state = np.append(next_state, float(noisyGame))\n",
    "            meta_next_state = (next_state, float(noisyGame))\n",
    "            replay_buffer.push(meta_state, original_action, reward, meta_next_state, done)\n",
    "\n",
    "            meta_state = meta_next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                noisyGame = 1-noisyGame\n",
    "                state = env.reset()\n",
    "                state = np.append(state, float(noisyGame))\n",
    "                meta_state = (state, float(noisyGame))\n",
    "                all_rewards.append(episode_reward)\n",
    "                episode_reward = 0\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                beta = beta_by_frame(frame_idx)\n",
    "                # TODO: to compare with PER paper, shouldn't this be every 4 frames?\n",
    "                loss = td_loss.compute_td_loss(current_model, target_model, beta, replay_buffer, optimizer)\n",
    "                losses.append(loss.data.tolist())\n",
    "\n",
    "            if frame_idx % 200 == 0:\n",
    "                all_standard_val_rewards.append(test(val_env, False, eps, num_val_trials, current_model))\n",
    "                all_proportions.append(float(replay_buffer.states_count[1]) / (float(replay_buffer.states_count[1])  + float(replay_buffer.states_count[0])))\n",
    "                weight_dict = replay_buffer.get_average_weight_by_env()\n",
    "                std_weights.append(weight_dict['std_avg'])\n",
    "                noisy_weights.append(weight_dict['noisy_avg'])\n",
    "                std_buffer_example_count.append(weight_dict['std_count'])\n",
    "                noisy_buffer_example_count.append(weight_dict['noisy_count'])\n",
    "                \n",
    "                # Verified that standard experience replay weights noisy examples more heavily and oversamples\n",
    "                # TODO: run complete experiments and check improvement with proposed replay technique\n",
    "#                 print(weight_dict)\n",
    "#                 print('Noisy to std weight', noisy_weights[-1]/std_weights[-1])\n",
    "#                 print('Proportion of noisy selected (long run): ', all_proportions[-1])\n",
    "                \n",
    "            #         plot(frame_idx, all_rewards, losses, standard_val_rewards, noisy_val_rewards, states_count_ratios)\n",
    "\n",
    "            if frame_idx % 1000 == 0:\n",
    "                update_target(current_model, target_model)\n",
    "                print(\"frame_idx\", frame_idx)\n",
    "\n",
    "            f.value += 1\n",
    "\n",
    "    result_df['trial_num'] = list(range(int(num_frames / 200))) * num_trials # 50 = 10000 frames / 200 frams\n",
    "    result_df['frame'] = result_df['trial_num'] * 200\n",
    "    result_df['val_reward'] = all_standard_val_rewards\n",
    "    result_df['proportion'] = all_proportions\n",
    "    result_df['std_weights'] = std_weights\n",
    "    result_df['noisy_weights'] = noisy_weights\n",
    "    result_df['std_buffer_example_count'] = std_buffer_example_count\n",
    "    result_df['noisy_buffer_example_count'] = noisy_buffer_example_count\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(variances = np.logspace(0.1, 10, 4), thetas = np.logspace(0.1, 5, 4), exps = [False], meg_norms = [True, False], hardcodeds=[True, False], average_q_values=[False]):\n",
    "    meta_dict = {}\n",
    "    #pd.DataFrame(columns=['var', 'theta', 'exp', 'meg_norm', 'result_df'])\n",
    "    for var in variances:\n",
    "        for theta in thetas:\n",
    "            for exp in exps:\n",
    "                for avg_q in average_q_values:\n",
    "                    for meg_norm in meg_norms:\n",
    "                        for hardcoded in hardcodeds:\n",
    "                            env_id = \"CartPole-v0\"\n",
    "                            env, val_env = get_env(env_id)\n",
    "                            result_df = train(env, val_env, var, theta, exp, avg_q, meg_norm, hardcoded, invert_actions = False, num_frames = 10000, num_val_trials = 10, batch_size = 32, num_trials = 5, USE_CUDA = USE_CUDA)\n",
    "                            # TODO: make more descriptive\n",
    "                            key = (var, theta, exp, avg_q, hardcoded)\n",
    "                            meta_dict[key] = result_df\n",
    "    return meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cde57668d34c1a97c8ae451513fcb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, max=10000.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial number: 0\n",
      "frame_idx 1000\n",
      "frame_idx 2000\n",
      "frame_idx 3000\n",
      "frame_idx 4000\n",
      "frame_idx 5000\n",
      "frame_idx 6000\n",
      "frame_idx 7000\n",
      "frame_idx 8000\n",
      "frame_idx 9000\n",
      "frame_idx 10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2cd4954e5e45b894c65c02518fa795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, max=10000.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial number: 0\n",
      "frame_idx 1000\n",
      "frame_idx 2000\n",
      "frame_idx 3000\n",
      "frame_idx 4000\n",
      "frame_idx 5000\n",
      "frame_idx 6000\n",
      "frame_idx 7000\n",
      "frame_idx 8000\n",
      "frame_idx 9000\n",
      "frame_idx 10000\n"
     ]
    }
   ],
   "source": [
    "# meta_dict = run_experiments(variances = np.logspace(0.1, 10, 4), \n",
    "#                             thetas = np.logspace(0.1, 5, 4), \n",
    "#                             exps = [False], \n",
    "#                             meg_norms = [True, False], \n",
    "#                             hardcodeds=[True, False])\n",
    "\n",
    "# TODO: restructure code for desired experiments\n",
    "meta_dict = run_experiments(variances = [0], \n",
    "                            thetas = [1], \n",
    "                            exps = [False], \n",
    "                            meg_norms = [False], \n",
    "                            hardcodeds=[False],\n",
    "                            average_q_values=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "curr_dt = datetime.datetime.now()\n",
    "save_dir = 'results/' + curr_dt.strftime('%m-%y-%d_%H:%M:%S.%f') + 'eps=.5var=0' + '/'\n",
    "print(save_dir)\n",
    "\n",
    "ms.save(meta_dict, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.load(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
